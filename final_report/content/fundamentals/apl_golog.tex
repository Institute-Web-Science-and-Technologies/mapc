\subsubsection[GOLOG]{GOLOG$^\circ$}\label{fun:apl_golog}
This section gives a summary of the logic programming language GOLOG.
Moreover, its problems in context of the \enquote{Agents on Mars} scenario are shown.
If not further specified, all information except for the examples is taken from Levesque et~al.~\cite{levesque_golog:_1997} who introduced the language.
GOLOG builds on the situation calculus.
To allow high-level programming, the language adds complex actions like loops, conditions, tests and non-deterministic elements.
As an example, a GOLOG program should have a robot pouring other agents coffee until everybody has coffee.
After that, the robot should sing and terminate.
Such a program would reuse the fluent of \autoref{f_hasCoffee}, the action precondition axioms of \autoref{a_possPourCoffee} and \ref{a_possSing}, the successor state axiom of \autoref{a_sucStateAxiom} and extend them with the two procedures given in \autoref{p_main} and \ref{p_pourSOCoffee}:
\begin{equation}\label{p_main}
  \begin{split}
    \textbf{proc}\ \texttt{main}\ [&\textbf{while}\ (\exists p) \neg\textit{hasCoffee}(p) \\
    &\textbf{do}\ \texttt{pourSOCoffee}(p)\ \textbf{endWhile}]; \\
    \textit{sing}&\ \textbf{endProc}.
  \end{split}
\end{equation}
\begin{equation}\label{p_pourSOCoffee}
  \begin{split}
    \textbf{proc}\ \texttt{pourSOCoffee}\ (\boldsymbol{\pi} p)\ [ &\neg\textit{hasCoffee}(p)\textbf{?}; \\
    &\textit{pourCoffee}(p)]\ \textbf{endProc}.
  \end{split}
\end{equation}
\autoref{p_main} shows the procedure which can be seen as the main method.
It loops as long as there exist agents without coffee and tells the robot to pour coffee for some agent which is lacking coffee.
After completion of its coffee-pouring task, the robot sings.
\autoref{p_pourSOCoffee} allows the robot to non-deterministically choose an agent $p$ to pour coffee for by using the $\pi$-operator.
The $?$-operator is similar to the \texttt{if}-operator in other programming languages like Java.
Due to the non-determinsmic operator, there can be two different resulting situations as shown in \autoref{ex_situations} with the initial configuration given in \autoref{ex_gologConfiguration}:
\begin{equation}\label{ex_gologConfiguration}
  \neg\textit{hasCoffee}(p,s_0) \Leftrightarrow p=\textrm{Jane} \vee p=\textrm{John}.
\end{equation}
\begin{equation}\label{ex_situations}
  \begin{split}
    s=\textit{do}\Big(\textit{sing},\textit{do}\big(&\textit{pourCoffee}(\textrm{Jane}),
      \textit{do}(\textit{pourCoffee}(\textrm{John}),s_0)\big)\Big),
\\  s=\textit{do}\Big(\textit{sing},\textit{do}\big(&\textit{pourCoffee}(\textrm{John}),
      \textit{do}(\textit{pourCoffee}(\textrm{Jane}),s_0)\big)\Big)
  \end{split}
\end{equation}

Levesque et~al.~\cite{levesque_golog:_1997} highlight multiple problems with GOLOG.
Problems which become relevant when considering applying GOLOG to the \enquote{Agents on Mars} scenario are given in this part.
One problem is that complete knowledge is assumed in the initial situation.
This is not the case for the \enquote{Agents on Mars} scenario and scenarios with unknown worlds that get explored by agents in general.

The second problem is that GOLOG does not offer a simple solution for sensing actions and reactions of agents on sensed actions.
Sensing actions are actions by agents that may not modify fluents but the internal knowledge of agents by detecting some properties in the world~\cite{thielscher_flux:_2005}.
This can be seen as a side-effect of GOLOG not being developed for unknown worlds.
Again, this would be a feature which is needed for the \enquote{Agents on Mars} scenario.

A third problem is that exogenous actions cannot be handled.
Exogenous actions are actions outside of the agent's control.
In the \enquote{Agents on Mars} scenario, this e.g.\ could be the loss of an agent's health due to an enemy agent attacking it.

Thielscher~\cite{thielscher_flux:_2005} highlights a fourth problem.
It arises from GOLOG being \emph{regression-based}.
This means that deciding whether an action is executable is only possible after looking at all previous actions and how they might have affected the world.
As a result, reasoning takes exponentially longer over time and hence GOLOG does not scale.
Due to these problems, GOLOG is unsuitable for a multiple agent-based scenario like the \enquote{Agents on Mars} scenario without considerable modifications and extensions.
